{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-fu_E1krMXh"
      },
      "outputs": [],
      "source": [
        "!pip install gradio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKdi5jUjrifE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JybHLbCFrw-4"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data.csv')\n",
        "df = df[['source_txt', 'plagiarism_txt', 'label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVwwIyZfrznG"
      },
      "outputs": [],
      "source": [
        "df = df.dropna(subset=['source_txt', 'plagiarism_txt', 'label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0efgNu6r2EE"
      },
      "outputs": [],
      "source": [
        "df['label'] = df['label'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnVY0nBXr4MH"
      },
      "outputs": [],
      "source": [
        "print(\"NaN in label\", df['label'].isna().sum())\n",
        "print(\"Unique labels\", df['label'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxNGLcrpr6-w"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)  # remove extra whitespace\n",
        "    text = re.sub(r'\\d+', '', text)   # remove digits\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y71f_oZCr9aD"
      },
      "outputs": [],
      "source": [
        "df['source_txt'] = df['source_txt'].astype(str).apply(clean_text)\n",
        "df['plagiarism_txt'] = df['plagiarism_txt'].astype(str).apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH-r-nIKsAQO"
      },
      "outputs": [],
      "source": [
        "df['combined'] = df['source_txt'] + ' [SEP] ' + df['plagiarism_txt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHrRoXLWsCwb"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "st_model = SentenceTransformer('all-MiniLM-L6-v2')  # ~80MB, fast and semantic\n",
        "\n",
        "source_embeddings = st_model.encode(df['source_txt'].tolist(), show_progress_bar=True)\n",
        "suspect_embeddings = st_model.encode(df['plagiarism_txt'].tolist(), show_progress_bar=True)\n",
        "\n",
        "import numpy as np\n",
        "X = np.hstack((source_embeddings, suspect_embeddings))\n",
        "\n",
        "y = df['label']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5yQGk_JsZlh"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxR441wxt231"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSiQybzpt8UP"
      },
      "outputs": [],
      "source": [
        "lr_model = LogisticRegression(max_iter=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi4W4bRut_LY"
      },
      "outputs": [],
      "source": [
        "lr_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRFAWUwduA0Q"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjYfEqIUuJdA"
      },
      "outputs": [],
      "source": [
        "lr_preds = lr_model.predict(X_test)\n",
        "print(\"üîç Logistic Regression Classification Report\")\n",
        "print(classification_report(y_test, lr_preds))\n",
        "cm_lr = confusion_matrix(y_test, lr_preds)\n",
        "disp_lr = ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=[\"Original\", \"Plagiarized\"])\n",
        "disp_lr.plot(cmap='Greens')\n",
        "plt.title(\"Logistic Regression Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQcDdMkfuNWF"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXfTPaIpzW-x"
      },
      "outputs": [],
      "source": [
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm-wrauY1IkV"
      },
      "outputs": [],
      "source": [
        "rf_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvuLEl5IuQSP"
      },
      "outputs": [],
      "source": [
        "# Evaluate\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "print(\"üîç Random Forest Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred_rf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va3W8kdGuSua"
      },
      "outputs": [],
      "source": [
        "cm_lr = confusion_matrix(y_test, lr_preds)\n",
        "disp_lr = ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=[\"Original\", \"Plagiarized\"])\n",
        "disp_lr.plot(cmap='Blues')\n",
        "plt.title(\"Random Forest tree matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SspuFrma8eBe"
      },
      "outputs": [],
      "source": [
        "lr_report = classification_report(y_test, lr_preds, output_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rtFcy5S8nd3"
      },
      "outputs": [],
      "source": [
        "rf_report = classification_report(y_test, y_pred_rf, output_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6ahzeyI9eRD"
      },
      "outputs": [],
      "source": [
        "comparison_df = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'Random Forest'],\n",
        "    'Accuracy': [lr_report['accuracy'], rf_report['accuracy']],\n",
        "    'Precision': [lr_report['weighted avg']['precision'], rf_report['weighted avg']['precision']],\n",
        "    'Recall': [lr_report['weighted avg']['recall'], rf_report['weighted avg']['recall']],\n",
        "    'F1-score': [lr_report['weighted avg']['f1-score'], rf_report['weighted avg']['f1-score']]\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMUZYke89gRK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming comparison_df is your DataFrame\n",
        "print(\"Model Comparison Results:\")\n",
        "display(comparison_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DLsd7rX-YQU"
      },
      "outputs": [],
      "source": [
        "def predict_plagiarism(source, suspect):\n",
        "    src_emb = st_model.encode([clean_text(source)])\n",
        "    sus_emb = st_model.encode([clean_text(suspect)])\n",
        "    combined_emb = np.hstack((src_emb, sus_emb))\n",
        "    pred = lr_model.predict(combined_emb)[0]\n",
        "    return \"Plagiarized\" if pred == 1 else \"Original\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict_plagiarism,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=5, label=\"Source Text\"),\n",
        "        gr.Textbox(lines=5, label=\"Suspect Text\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Plagiarism Detector (Sentence-BERT)\",\n",
        "    description=\"Detect plagiarism using semantic similarity from Sentence Transformers (MiniLM).\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}